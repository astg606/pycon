{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-24vit7OoB1x"
   },
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h2><font color= \"blue\" size=\"+3\">PyCon 2024 Tutorial</font></h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h3>Python Workflows to Extract and Plot Satellite Data Products along Tracks</h3>\n",
    "    <h2><font color=\"red\" size=\"+3\">Tracking the Movement of the Aura Satellite</font></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqZRoZwmoB11"
   },
   "source": [
    "----\n",
    "[Jules Kouatchou](mailto:Jules.Kouatchou@nasa.gov) • [Bruce Van Aartsen](mailto:bruce.vanaartsen@nasa.gov)\n",
    "-\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Objectives</font>\n",
    "\n",
    "We want to use a timeseries collections of data files from the Aura satellite to:\n",
    "- Track the movement of the satellite\n",
    "- Plot along the path values of fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> NASA Earth Observing System (EOS)</font>\n",
    "\n",
    "![fig_eos](https://eospso.nasa.gov/sites/default/files/u15/fleet-swoosh-082922_Sept27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">EOS</font>\n",
    "\n",
    "- Was conceived in the 1980s and began to take shape in the early 1990s. \n",
    "- Is comprised of a series of coordinated polar-orbiting satellites designed to monitor and understand key components of the climate system and their interactions through long-term global observations.\n",
    "- The EOS missions focus on the following climate science areas:\n",
    "  - Radiation, clouds, water vapor, and precipitation,\n",
    "  - The oceans,\n",
    "  - Greenhouse gases; land-surface hydrology and ecosystem processes,\n",
    "  - Glaciers, sea ice, and ice sheets,\n",
    "  - Ozone and stratospheric chemistry, and\n",
    "  - Natural and anthropogenic aerosols.\n",
    "- The primary goal is to determine the extent, causes, and regional consequences of global climate change. \n",
    "- Some of the satellites included in EOS are:\n",
    "   - [Aqua](https://eospso.nasa.gov/missions/aqua): Collects data on the Earth's water cycle, including evaporation from the oceans, water vapor in the atmosphere, clouds,precipitation, soil moisture, sea ice, land ice, and snow cover on the land and ice.\n",
    "   - [Aura](https://eospso.nasa.gov/missions/aura): Is meant to study the atmosphere's chemistry and dynamics. Its measurements help investigate questions about ozone trends, air-quality changes, and their linkage to climate change. \n",
    "   - [Terra](https://eospso.nasa.gov/missions/terra): Explores the connections between Earth’s atmosphere, land, snow and ice, ocean, and energy balance to understand Earth’s climate and climate change and to map the impact of human activity and natural disasters on communities and ecosystems\n",
    "   - [Landsat 7](https://eospso.nasa.gov/missions/landsat-7): Gathers remotely sensed images of land surface and coastal regions for global change research, regional environmental change\n",
    "studies, national security uses, and other civil and commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Ozone Monitoring Instrument (OMI)</font>\n",
    "\n",
    "### <font color=\"blue\">Overview</font>\n",
    "- [Ozone Monitoring Instrument (OMI)](https://aura.gsfc.nasa.gov/omi.html) is key instrument aboard the Aura satellite.\n",
    "  - The Aura satellite orbits at an altitude of 705 km in a sun-synchronous polar orbit with an exact 16-day repeat cycle.\n",
    "  - OMI is a nadir-viewing wide-field-imaging spectrometer, giving daily global coverage.\n",
    "  - OMI measures the key air quality components such as nitrogen dioxide (NO$_2$), sulfur dioxide (SO$_2$), bromine oxide (BrO), OClO, and aerosol characteristics.\n",
    "  - OMI provides mapping of pollution products from an urban to super-regional scale.\n",
    "- The US Environmental Protection Agency (EPA) has designated the atmospheric constituents (such as $O_3$, $NO_2$, $SO_2$) measured by OMI as posing serious threats to human health and agricultural productivity. These measurements are made at near urban scale resolution and track industrial pollution and biomass burning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">OMI Swath</font>\n",
    "- One OMI swath is measured every two seconds.\n",
    "- Due to the optical characteristics of the instrument, adjacent swaths overlap considerably in their ground coverage.\n",
    "- The width of a swath ensures that swaths from adjacent consecutive orbits are nearly contiguous at the equator and have some overlap at mid- to high-latitudes.\n",
    "-  In a single orbit, OMI measures approximately 1650 swaths from terminator to terminator.\n",
    "-  In the standard global measurement mode, OMI observes __60__ ground pixels (13 km x 24 km at nadir) across the swath (13 km x 48 km at nadir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Satellite acquiring image at nadir\n",
    "\n",
    "![fig_sat](https://images.ctfassets.net/nzn0tepgtyr1/6QMhOl5l5tIrQobVDloRuh/8ff2446a3e312452c6671e8792e0710f/image19_1.png)\n",
    "Image Source: [Antonio Almeida, How to perform orthorectification: A practical guide](https://up42.com/blog/how-to-perform-orthorectification-a-practical-guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">OMI Data</font>\n",
    "\n",
    "The OMI data are available in the following four levels: \n",
    "\n",
    "- __Level 0__ products are raw sensor counts. Level 0 data are packaged into two-hour \"chunks\" of observations in the life of the spacecraft  irrespective of orbital boundaries. They contain orbital swath data.\n",
    "- __Level 1B__ processing takes Level 0 data and calibrates, geo-locates and packages the data into orbits. They contain orbital swath data.\n",
    "- <font color=\"green\">__Level 2__ products contain orbital swath data.</font>\n",
    "- __Level 3__ products contain global data that are composited over time (daily or monthly) or over space for small equal angle (latitude longitude) grids covering the whole globe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we focus on the [Nitrogen Dioxide ($NO_2$) Total and Tropospheric Column](https://disc.gsfc.nasa.gov/datasets/OMNO2_003/summary) 1-orbit L2 Swath.\n",
    "\n",
    "- Although only a small percentage  of $NO_2$ is formed naturally by lightning, volcanoes, water, and bacterial activity on soil and plants, it can also be emitted through the burning of biomass.\n",
    "- In urban areas, $NO_2$ originates mostly from the combustion of fossil fuels from a large variety of activities, such as electricity generation, manufacturing industries, food processing, transportation, among others.\n",
    "- $NO_2$ is an important chemical species in both the stratosphere (where it plays a key role in ozone chemistry) and in the troposphere (where it is a precursor to ozone production).\n",
    "- In the troposphere, it has a lifetime of several hours, is an important contributor to air pollution and can adversely affect human health.\n",
    "   - Human exposure to elevated $NO_2$ concentrations is associated with a range of adverse outcomes such as respiratory infections, increases in asthma incidence, lung cancer and overall mortality. \n",
    "   - The effect is considerable in large urban centers since the people are exposed to this pollutant for long periods of time.\n",
    "- The OMI $NO_2$ data products are infered from the measurements, with the minimum possible dependence on model simulations.\n",
    "   - They have been used widely for a large variety of purposes, especially for air quality monitoring.\n",
    "   - Their long-time series makes it valuable when the evolution of this important pollutant needs to be tracked throughout a longer period of time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">How to Obtain OMI Data</font>\n",
    "\n",
    "- For this tutorial, we have already obtained data files for July 9, 2023.\n",
    "- In case you want to obtain data on your own, you can visit NASA's EARTH<b>DATA</b> website.\n",
    "Existing users can [login here](https://urs.earthdata.nasa.gov/home). New users must click the REGISTER button.\n",
    "<br>\n",
    "\n",
    "![fig_login](https://ghrc.nsstc.nasa.gov/home/sites/default/files/fh4553/LoginPage.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After logging in, click the EARTH<b>DATA</b> logo in the top left to go to the home page.\n",
    "- On the home page, click on the \"Data\" link at the top, and then select \"Earthdata Search\", which will bring you to a page similar to this:\n",
    "![fig_search](https://www.earthdata.nasa.gov/s3fs-public/2021-10/cmr_screenshot.png?VersionId=dCqvkMUioPt6g8FWbog8VZA25xDj_imB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To find the OMI collections, enter \"OMI NO2\" in the Search box on the top left. One of the resulting choices should show \"OMI/Aura Nitrogen Dioxide (NO2) Total and - Tropospheric Column 1-orbit L2 Swath 13x24 km\". \n",
    "- Click on that box to reveal the list of data \"Granules\" available in the collection, which will resemble this:\n",
    "![fig_granules](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/tutorials/img/select_granules_download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can use the filters menu at left to narrow down the times or location you are interested in. Click the \"+\" sign inside the box of each individual granule you'd like to download. It will turn to a \"-\" sign to indicate it has been selected. Then click the green Download button to get all the selected granules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLTjzNBJoB12"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYmagjSHoB13"
   },
   "source": [
    "## Required Packages\n",
    "\n",
    "\n",
    "- __Matplotlib__: for basic plots.\n",
    "- __Pandas__: Manipulation and exploratory data analysis of tabular data.\n",
    "- __Shapely__: For manipulation and analysis of planar geometric objects\n",
    "- __GeosPandas__: Combines the capabilities of Pandas and Shapely for geospatial operations\n",
    "- __MovingPandas__: Handling the movement of geospatial objects.\n",
    "- __h5py__: Read HDF5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nT_3Q7CUoB15"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX-VYGqaoB16"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBT2ouX8oB16"
   },
   "outputs": [],
   "source": [
    "from shapely import geometry as shpgeom\n",
    "from shapely import wkt as shpwkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import movingpandas as mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_defaults = {'linewidth':5, 'capstyle':'round', 'figsize':(9,3), 'legend':True}\n",
    "hv.opts.defaults(hv.opts.Overlay(active_tools=['wheel_zoom'], \n",
    "                              frame_width=500, frame_height=400))\n",
    "hvplot_defaults = {'tiles':None, 'cmap':'Viridis', 'colorbar':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color='red'>h5py</font>\n",
    "- Python interface to the HDF5 binary data format.\n",
    "- Provide easy-to-use high level interface, which allows you to store huge amounts of numerical data.\n",
    "- Easily manipulate that data from NumPy.\n",
    "    - Use straightforward NumPy and Python metaphors, like dictionary and NumPy array syntax.\n",
    "- Within h5py, HDF5 groups work like dictionaries, and datasets work like NumPy arrays.\n",
    "\n",
    "It is important to know the structure of an HDF5 file. It can be as a container for two kinds of objects: \n",
    "1. **Datasets**:, Array-like collections of data.\n",
    "2. **Groups**: Folder-like containers that hold datasets and other groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">Reading a sample data file</font>\n",
    "\n",
    "We have a collection of data files that have timeseries observations.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/Users/jkouatch/myTasks/PythonTraining/ASTG606/Materials/sat_data/OMI_Data/\"\n",
    "data_dir = \"/tljh-data/sat_data/OMI_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files =[\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0114-o100959_v003-2023m0710t052026.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0253-o100960_v003-2023m0710t052055.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0432-o100961_v003-2023m0710t060000.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0610-o100962_v003-2023m0710t124018.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0749-o100963_v003-2023m0710t141856.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0928-o100964_v003-2023m0710t141539.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1107-o100965_v003-2023m0710t143421.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1246-o100966_v003-2023m0710t171304.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1425-o100967_v003-2023m0710t171303.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1603-o100968_v003-2023m0710t171256.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1742-o100969_v003-2023m0710t171227.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1921-o100970_v003-2023m0710t224725.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t2100-o100971_v003-2023m0710t224852.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t2239-o100972_v003-2023m0710t224703.he5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each file represents a swath.\n",
    "- When we do the analysis, we need to treat files individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = Path(data_dir) / list_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all groups, datasets and their attributes: use `visititems` (mimicking `h5ls`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attrs(name, obj):\n",
    "    \"\"\"\n",
    "    Given a name and the associated HDF5 object (group or dataset),\n",
    "    print the name and the metadata of the object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name of the object\n",
    "    obj : \n",
    "        HDF5 object (can be either group or dataset)\n",
    "    \"\"\"\n",
    "    shift = name.count('/') * '\\t'\n",
    "    print(f\"{shift} {name}\")\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{shift} \\t Shape: {obj[()].shape}\")\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(f\"{shift} \\t {key}: {val}\")\n",
    "        \n",
    "with h5py.File(fname, mode='r') as h5f:\n",
    "    h5f.visititems(print_attrs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The file has groups organized in a directory tree.\n",
    "   - The file attributes are in `HDFEOS/ADDITIONAL/`.\n",
    "   - The main group associated with measurements is `HDFEOS/SWATHS/ColumnAmountNO2/Data Fields`.\n",
    "   - The main group associated with geolocated information is `HDFEOS/SWATHS/ColumnAmountNO2/Geolocation Fields`.\n",
    "      - We have here the parameters `SpacecraftLatitude` and `SpacecraftLongitude` that correspond to subsatellite point, i.e., the point on the Earth's surface directly below the satellite.\n",
    "- Each dataset has the attributes: `ScaleFactor`, `Offset`, `MissingValue` and `_FillValue`. __We will need to use these attributes to restore and mask the values of the dataset before doing analyses.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you get a specific information from the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname, mode='r') as h5f:\n",
    "    file_attr = dict(h5f['HDFEOS/ADDITIONAL/FILE_ATTRIBUTES'].attrs)\n",
    "    geol_group = h5f['HDFEOS']['SWATHS']['ColumnAmountNO2']['Geolocation Fields']\n",
    "    data_group = h5f['HDFEOS/SWATHS/ColumnAmountNO2/Data Fields']\n",
    "    tropo = data_group['TropopausePressure'][()]\n",
    "    time = geol_group['Time'][()]\n",
    "    lats = geol_group['Latitude'][()]\n",
    "    lons = geol_group['Longitude'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict(file_attr):\n",
    "    print(f\"{key}: {file_attr[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of tropo:     {tropo.shape}\")\n",
    "print(f\"Shape of time:      {time.shape}\")\n",
    "print(f\"Shape of latitude:  {lats.shape}\")\n",
    "print(f\"Shape of longitude: {lons.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1644__ corresponds to the number of time records and __60__ to number of ground pixels per observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time[0:3])\n",
    "print(time[-4:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground pixel index\n",
    "# 29: OMI’s smallest pixel is at the center of swath\n",
    "ipxl = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lats[0:3, ipxl])\n",
    "print(lats[-4:-1, ipxl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lons[0:3, ipxl])\n",
    "print(lons[-4:-1, ipxl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">Supporting functions</font>\n",
    "\n",
    "- Each dataset (field) in the file comes with attributes such as `ScaleFactor`, `Offset`, `MissingValue` and `_FillValue`.\n",
    "- After we read a dataset (a a NumPy array), we need to restore its values:\n",
    "```\n",
    "   data = NaN where data == MissingValue\n",
    "   data = NaN where data == _FillValue\n",
    "   data = (data - Offset)*ScaleFactor\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "We need to write functions to extract the dataset atributes and perform the above operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_dtype(sample_dict):\n",
    "    '''\n",
    "    Converts attribute dictionary from NumPy data types \n",
    "    to general Python data types\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_dict : dict\n",
    "         A dictionary of attributes\n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    sample_dict : dict\n",
    "         A dictionary of attributes\n",
    "    '''\n",
    "    for key, item in sample_dict.items():\n",
    "        if isinstance(item, np.ndarray):   # Converts np arrays to a list to, if applicable, an int or float\n",
    "            item = list(item)\n",
    "        \n",
    "            if len(item) == 1:\n",
    "                item = item[0]\n",
    "        elif isinstance(item, np.bytes_):   # Converts np bytes to an np string to a Python string\n",
    "            item = str(item.astype('str'))\n",
    "        \n",
    "            if item[0] == '(' or item[0] == '{':   # Converts to tuple or dict if applicable\n",
    "                item = eval(item)\n",
    "            # **eval() relaiability??**\n",
    "            \n",
    "        sample_dict[key] = item   # Updates any changes to the key value\n",
    "        \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_attrs(ds):\n",
    "    \"\"\"\n",
    "    Given a dataset identifier, return the dataset attribute.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : \n",
    "       dataset identifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ds_attrs : dict\n",
    "       A dictionary of the dataset attributes.\n",
    "    \"\"\"\n",
    "    ds_attrs = dict(ds.attrs)\n",
    "    ds_attrs = convert_dict_dtype(ds_attrs)\n",
    "    \n",
    "    return ds_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_attribute_value(ds_attrs, attr_name):\n",
    "    '''\n",
    "    Obtain the value of a specified attribute in a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds_attrs : dict\n",
    "         A dictionary of dataset attributes\n",
    "    attr_name : str\n",
    "         Attribute name    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    value : float, int, str, list\n",
    "         Value of the attribute. If attribute not available, None.\n",
    "    '''\n",
    "    for key, value in ds_attrs.items():\n",
    "        if key == attr_name:\n",
    "            return value \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(ds):\n",
    "    '''\n",
    "    Restore the dataset data using the dataset attributes.\n",
    "\n",
    "       data = NaN where data == MissingValue\n",
    "       data = NaN where data == _FillValue\n",
    "       data = (data - Offset)*ScaleFactor\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : \n",
    "       h5py dataset identifier\n",
    "    \n",
    "    Returns:\n",
    "    data : numpy array\n",
    "       NumPy array for the restored data\n",
    "    '''\n",
    "    ds_attrs = get_ds_attrs(ds)\n",
    "    \n",
    "    fill_value = get_ds_attribute_value(ds_attrs, '_FillValue')\n",
    "    missing_value = get_ds_attribute_value(ds_attrs, 'MissingValue')\n",
    "    scale_factor = get_ds_attribute_value(ds_attrs, 'scale_factor')\n",
    "    add_offset = get_ds_attribute_value(ds_attrs, 'add_offset')\n",
    "    \n",
    "    data = ds[()]#.astype('float')\n",
    "\n",
    "    data = np.where(data != missing_value, data, np.nan)\n",
    "    data = np.where(data != fill_value, data, np.nan)\n",
    "    if add_offset:\n",
    "        data -= add_offset\n",
    "    if scale_factor:\n",
    "        data *= scale_factor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">Function for reading specific data from a file</font>\n",
    "\n",
    "We write a function that takes a file name and returns the folloing arrays:\n",
    "- time records\n",
    "- latitude values\n",
    "- longitude values\n",
    "- `ColumnAmountNO2Trop` values\n",
    "\n",
    "The above data arrays will be used to create a timeseries dataset for the field `ColumnAmountNO2Trop`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(fname, ipxl):\n",
    "    with h5py.File(fname, 'r') as fid:\n",
    "        geol_grp = fid['HDFEOS']['SWATHS']['ColumnAmountNO2']['Geolocation Fields']\n",
    "        data_grp = fid['HDFEOS']['SWATHS']['ColumnAmountNO2']['Data Fields']\n",
    "        NO2 = restore_data(data_grp['ColumnAmountNO2Trop'])[:,ipxl]\n",
    "        time = geol_grp['Time'][()]\n",
    "        lats = geol_grp['Latitude'][()][:,ipxl]\n",
    "        lons = geol_grp['Longitude'][()][:,ipxl]\n",
    "    return NO2, time, lats, lons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"red\">Create the Pandas DataFrame</font>\n",
    "\n",
    "- We read all the files to get in each of them the time, laitude, longitude and NO2 arrays.\n",
    "- The gathered data are stacked in NumPy arrays.\n",
    "- Since each file represents a swath, we will assign an index to distinguish data from each file. A addition NumPy arrays is used to store the indices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = len(list_files)\n",
    "ipxl = 10\n",
    "first_iter = True\n",
    "for i in range(num_files):\n",
    "    fname = Path(data_dir) / list_files[i]\n",
    "    print(f\"Reading: {fname}\")\n",
    "    X, Y, Z, W = get_arrays(fname, ipxl)\n",
    "    traj = np.full_like(Y, i+1, dtype=int) # Index to distinguish files.\n",
    "    if first_iter:\n",
    "        first_iter = False\n",
    "        NO2, time, lats, lons = X, Y, Z, W\n",
    "        traj_id = traj\n",
    "    else:\n",
    "        NO2 = np.concatenate((NO2, X), axis=0)\n",
    "        time = np.concatenate((time, Y), axis=0)\n",
    "        lats = np.concatenate((lats, Z), axis=0)\n",
    "        lons = np.concatenate((lons, W), axis=0)\n",
    "        traj_id = np.concatenate((traj_id, traj), axis=0)\n",
    "\n",
    "X, Y, Z, W, traj = None, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform verifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the time (GPS unit) to a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = np.zeros_like(time, object)\n",
    "gps_epoch = dt.datetime(1980, 1, 6)\n",
    "for j, t in enumerate(time):\n",
    "    Times[j] = (gps_epoch + dt.timedelta(seconds=time[j] - (35 - 19))).strftime(\"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_name = \"NO2TropData\"\n",
    "\n",
    "df_omi = pd.DataFrame(\n",
    "    {\n",
    "        \"latitude\": lats, \n",
    "        \"longitude\": lons, \n",
    "        field_name: NO2, \n",
    "        \"t\": Times, \n",
    "        \"traj_id\": traj_id\n",
    "    }\n",
    ")\n",
    "\n",
    "df_omi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information on the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to have basic statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many individual `traj_id` do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Values of traj_id: \\n\\t {df_omi['traj_id'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to shift longitude from [-180,180] to [0,360]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_omi['longitude'] = df_omi['longitude']%360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <font color=\"red\">Create the MovingPandas trajectories</font>\n",
    "\n",
    "- Because we deal with several swaths, each of them is seen as a trajectory.\n",
    "- The MovingPandas function `TrajectoryCollection` is used to identify each swath.\n",
    "\n",
    "The general syntax for calling the function is:\n",
    "```\n",
    "TrajectoryCollection(data,\n",
    "                     traj_id_col=None,\n",
    "                     obj_id_col=None,\n",
    "                     t=None, x=None, y=None,\n",
    "                     crs='epsg:4326',\n",
    "                     min_length=0, min_duration=None)\n",
    "```\n",
    "\n",
    "In our case:\n",
    "\n",
    "- `data`: is the Pandas DataFrame `df_omi`.\n",
    "- `t`: the column with the datetime objects\n",
    "- `x`: the longitude values\n",
    "- `y`: the latitude values\n",
    "- `traj_id`: the indices of individual swaths.\n",
    "- `crs`: Setting of the Coordinate Reference System (CRS). It is set default values because the latitude/longitude values correspond to WGS84 CRS that is `epsg:4326`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi = mpd.TrajectoryCollection(df_omi,\n",
    "                          traj_id_col=traj_id,\n",
    "                          x = \"longitude\", y=\"latitude\",\n",
    "                          t=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrajs = len(traj_omi.trajectories)\n",
    "print(f\"Number of trajectories: {ntrajs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## <font color=\"red\">Visualization</font>\n",
    "\n",
    "Now that we have the MovingPandas trajectory collection, we can visualize the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi.plot(x='t', y='NO2TropData')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi[field_name].plot(kind='hist', figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot all the trajectories (swaths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can distinguish the trajectories by adding colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(mcolors.CSS4_COLORS.keys())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "\n",
    "for i in range(ntrajs):\n",
    "    traj_omi.trajectories[i].plot(ax=ax, \n",
    "                                  color=colors[i+4], lw=3,\n",
    "                                  label=f\"Trajectory: {i+1}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi[df_omi['traj_id']==2].latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi[df_omi['traj_id']==3].latitude.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focus on one trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_traj = 3\n",
    "my_traj = traj_omi.trajectories[id_traj]\n",
    "my_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_traj.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the trajectorory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_traj.plot(color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider taking a GeoPandas dataset for mapping the globe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the map of the world as background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15,10))\n",
    "base = world.plot(ax=ax, color='white', edgecolor='black',)\n",
    "my_traj.plot(ax=base, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add field values along the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "base = world.plot(ax=ax, color='white', edgecolor='black',)\n",
    "my_traj.plot(legend=True, \n",
    "             column=field_name, \n",
    "             capstyle='round', \n",
    "             cmap=\"jet\", ax=base);\n",
    "#plt.savefig(\"fig_ColumnAmaountNO2Trop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an interactive plot with `hvplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot = my_traj.df[field_name].hvplot()\n",
    "my_plot.opts(xrotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_traj.df.hvplot(x=\"longitude\", y=\"latitude\", \n",
    "                  c=field_name, \n",
    "                  cmap='jet', \n",
    "                  tiles=True, \n",
    "                  global_extent=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv3dOk6Frrqn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "introduction_geopandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
