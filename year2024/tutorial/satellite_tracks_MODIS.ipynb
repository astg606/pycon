{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-24vit7OoB1x"
   },
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h2><font color= \"blue\" size=\"+3\">PyCon 2024 Tutorial</font></h2>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h3>Python Workflows to Extract and Plot Satellite Data Products along Tracks</h3>\n",
    "    <h2><font color=\"red\" size=\"+3\">Tracking the Movement of the Aura Satellite</font></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqZRoZwmoB11"
   },
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Objectives</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLTjzNBJoB12"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYmagjSHoB13"
   },
   "source": [
    "## Required Packages\n",
    "\n",
    "\n",
    "- __Matplotlib__: for basic plots.\n",
    "- __Pandas__: Manipulation and exploratory data analysis of tabular data.\n",
    "- __Shapely__: For manipulation and analysis of planar geometric objects\n",
    "- __GeosPandas__: Combines the capabilities of Pandas and Shapely for geospatial operations\n",
    "- __MovingPandas__: Handling the movement of geospatial objects.\n",
    "- __h5py__: Read HDF5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nT_3Q7CUoB15"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX-VYGqaoB16"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBT2ouX8oB16"
   },
   "outputs": [],
   "source": [
    "from shapely import geometry as shpgeom\n",
    "from shapely import wkt as shpwkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import movingpandas as mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_defaults = {'linewidth':5, 'capstyle':'round', 'figsize':(9,3), 'legend':True}\n",
    "hv.opts.defaults(hv.opts.Overlay(active_tools=['wheel_zoom'], \n",
    "                              frame_width=500, frame_height=400))\n",
    "hvplot_defaults = {'tiles':None, 'cmap':'Viridis', 'colorbar':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd.show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Measurement of `NO2` by the Ozone Monitoring Instrument (OMI)</font>\n",
    "\n",
    "- [The Ozone Monitoring Instrument (OMI)](https://www.earthdata.nasa.gov/learn/find-data/near-real-time/omi) aboard NASA's Aura satellite (launched in 2004) measures ozone from Earth's surface to top-of-atmosphere. \n",
    "  - OMI is a nadir-viewing wide-field-imaging spectrometer, giving daily global coverage.\n",
    "  - OMI measures the key air quality components such as nitrogen dioxide (NO$_2$), sulfur dioxide (SO$_2$), bromine oxide (BrO), OClO, and aerosol characteristics.\n",
    "  - OMI provides mapping of pollution products from an urban to super-regional scale.\n",
    "- Near real-time (NRT) OMI data are available through LANCE generally within three hours after a satellite observation.\n",
    "\n",
    "Here we focus on the [Nitrogen Dioxide (NO2) Total and Tropospheric Column](https://disc.gsfc.nasa.gov/datasets/OMNO2_003/summary) 1-orbit L2 Swath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> What is HDF5?</font>\n",
    "* HDF5 is a file format and library for storing scientific data. \n",
    "* It supports files larger than 2 GB and  parallel I/O. \n",
    "* An HDF5 file is a container for two kinds of objects: \n",
    "   1. **Datasets**:, Array-like collections of data.\n",
    "   2. **Groups**: Folder-like containers that hold datasets and other groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/jkouatch/myTasks/PythonTraining/ASTG606/Materials/sat_data/OMI_Data/\"\n",
    "#data_dir = \"/tljh-data/sat_data/OMI_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files =[\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0114-o100959_v003-2023m0710t052026.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0253-o100960_v003-2023m0710t052055.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0432-o100961_v003-2023m0710t060000.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0610-o100962_v003-2023m0710t124018.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0749-o100963_v003-2023m0710t141856.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t0928-o100964_v003-2023m0710t141539.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1107-o100965_v003-2023m0710t143421.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1246-o100966_v003-2023m0710t171304.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1425-o100967_v003-2023m0710t171303.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1603-o100968_v003-2023m0710t171256.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1742-o100969_v003-2023m0710t171227.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t1921-o100970_v003-2023m0710t224725.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t2100-o100971_v003-2023m0710t224852.he5\",\n",
    "    \"OMI-Aura_L2-OMNO2_2023m0709t2239-o100972_v003-2023m0710t224703.he5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = Path(data_dir) / list_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all the datasets and their attributes: use `visititems` (mimicking `h5ls`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname2 = \"/Users/jkouatch/Downloads/TROPESS_OMI-Aura_L2_Standard_O3_20240101_MUSES_R1p22_FS_F0p9_J1.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attrs(name, obj):\n",
    "    shift = name.count('/') * '    '\n",
    "    print(shift + name)\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(shift + '    ' + f\"Shape: {obj[()].shape}\")\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(shift + '    ' + f\"{key}: {val}\")\n",
    "        \n",
    "with h5py.File(fname, mode='r') as fid:\n",
    "    fid.visititems(print_attrs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do you get a specific information from the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname, mode='r') as fid:\n",
    "    geo_group = fid['HDFEOS']['SWATHS']['ColumnAmountNO2']['Geolocation Fields']\n",
    "    data_group = fid['HDFEOS/SWATHS/ColumnAmountNO2/Data Fields']\n",
    "    tropo = data_group['TropopausePressure'][()]\n",
    "    time = geo_group['Time'][()]\n",
    "    lats = geo_group['SpacecraftLatitude'][()]\n",
    "    lons = geo_group['SpacecraftLongitude'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of tropo:     {tropo.shape}\")\n",
    "print(f\"Shape of time:      {time.shape}\")\n",
    "print(f\"Shape of latitude:  {lats.shape}\")\n",
    "print(f\"Shape of longitude: {lons.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_dtype(sample_dict):\n",
    "    '''\n",
    "    Converts attribute dictionary from NumPy data types \n",
    "    to general Python data types\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_dict : dict\n",
    "         A dictionary of attributes\n",
    "         \n",
    "    Returns\n",
    "    sample_dict : dictt\n",
    "         A dictionary of attributes\n",
    "    '''\n",
    "    for key, item in sample_dict.items():\n",
    "        if isinstance(item, np.ndarray):   # Converts np arrays to a list to, if applicable, an int or float\n",
    "            item = list(item)\n",
    "        \n",
    "            if len(item) == 1:\n",
    "                item = item[0]\n",
    "        elif isinstance(item, np.bytes_):   # Converts np bytes to an np string to a Python string\n",
    "            item = str(item.astype('str'))\n",
    "        \n",
    "            if item[0] == '(' or item[0] == '{':   # Converts to tuple or dict if applicable\n",
    "                item = eval(item)\n",
    "            # **eval() relaiability??**\n",
    "            \n",
    "        sample_dict[key] = item   # Updates any changes to the key value\n",
    "        \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_attrs(ds):\n",
    "    \"\"\"\n",
    "       Give a dataset identifier, return the dataset attribute.\n",
    "       \n",
    "       Input Parameters:\n",
    "          - ds: dataset identifier\n",
    "       Returned value:\n",
    "          - ds_attrs: a dictionary\n",
    "    \"\"\"\n",
    "    ds_attrs = dict(ds.attrs)\n",
    "    ds_attrs = convert_dict_dtype(ds_attrs)\n",
    "    \n",
    "    return ds_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_attribute_value(ds_attrs, attr_name):\n",
    "    '''\n",
    "    Obtain the value of a specified attribute in a dataset.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    ds_attrs : dict\n",
    "         A dictionary of dataset attributes\n",
    "    attr_name : str\n",
    "         Attribute name    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    value: float, int, str, list\n",
    "         Value of the attribute. If attribute not available, None.\n",
    "    '''\n",
    "    for key, value in ds_attrs.items():\n",
    "        if key == attr_name:\n",
    "            return value \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(ds):\n",
    "    '''\n",
    "    Restore the dataset data using the dataset attributes.\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : h5py dataset identifier\n",
    "    \n",
    "    Returns:\n",
    "    data : numpy array\n",
    "    '''\n",
    "    ds_attrs = get_ds_attrs(ds)\n",
    "    \n",
    "    _FillValue = get_ds_attribute_value(ds_attrs, '_FillValue')\n",
    "    scale_factor = get_ds_attribute_value(ds_attrs, 'scale_factor')\n",
    "    add_offset = get_ds_attribute_value(ds_attrs, 'add_offset')\n",
    "    \n",
    "    data = ds[()]#.astype('float')\n",
    "    \n",
    "    data = np.where(data != _FillValue, data, np.nan)\n",
    "    if add_offset:\n",
    "        data -= add_offset\n",
    "    if scale_factor:\n",
    "        data *= scale_factor\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(fname):\n",
    "    with h5py.File(fname, 'r') as fid:\n",
    "        geo_grp = fid['HDFEOS']['SWATHS']['ColumnAmountNO2']['Geolocation Fields']\n",
    "        data_grp = fid['HDFEOS']['SWATHS']['ColumnAmountNO2']['Data Fields']\n",
    "        #NO2 = restore_data(data_grp['ColumnAmountNO2Trop'])[:,0]\n",
    "        NO2 = restore_data(data_grp['TropopausePressure'])[:,0]\n",
    "        time = geo_grp['Time'][()]\n",
    "        lats = geo_grp['SpacecraftLatitude'][()]\n",
    "        lons = geo_grp['SpacecraftLongitude'][()]\n",
    "    return NO2, time, lats, lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = len(list_files)\n",
    "first_iter = True\n",
    "for i in range(1):\n",
    "    fname = Path(data_dir) / list_files[i]\n",
    "    print(f\"Reading: {fname}\")\n",
    "    X, Y, Z, W = get_arrays(fname)\n",
    "    if first_iter:\n",
    "        first_iter = False\n",
    "        NO2, time, lats, lons = X, Y, Z, W\n",
    "    else:\n",
    "        NO2 = np.concatenate((NO2, X), axis=0)\n",
    "        time = np.concatenate((time, Y), axis=0)\n",
    "        lats = np.concatenate((lats, Z), axis=0)\n",
    "        lons = np.concatenate((lons, W), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the time (GPS unit) to a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = np.zeros_like(time, object)\n",
    "gps_epoch = dt.datetime(1980, 1, 6)\n",
    "for j, t in enumerate(time):\n",
    "    Times[j] = (gps_epoch + dt.timedelta(seconds=time[j] - (35 - 19))).strftime(\"%Y-%m-%d %H:%M:%S.%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi = pd.DataFrame(\n",
    "    dict(latitude=lats, longitude=lons, \n",
    "         NO2TropSurf=NO2, t=Times))\n",
    "df_omi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_omi = df_omi.set_index('t')\n",
    "#df_omi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi['longitude'] = df_omi['longitude']%360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi.plot(x='t', y='NO2TropSurf')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_omi['NO2TropSurf'].plot(kind='hist', figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi = mpd.Trajectory(df_omi,\n",
    "                          traj_id=1,\n",
    "                          x = \"longitude\", y=\"latitude\",\n",
    "                          t=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "traj_omi.plot(legend=True, \n",
    "           column=\"NO2TropSurf\", \n",
    "           capstyle='round', \n",
    "              cmap=\"jet\", ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_omi.hvplot(tiles=\"ESRI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_kwargs = dict(hover_cols=[\"latitude\", \"longitude\"], frame_height=300, frame_width=300)\n",
    "\n",
    "traj_omi.hvplot(**hv_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wv3dOk6Frrqn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "introduction_geopandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
